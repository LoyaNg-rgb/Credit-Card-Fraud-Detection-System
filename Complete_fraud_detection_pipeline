"""
Complete Fraud Detection Pipeline - End-to-End Notebook
Author: Loyanganba Ngathem
GitHub: github.com/LoyaNg-rgb

This notebook demonstrates the complete fraud detection workflow:
1. Data Loading & EDA
2. Feature Engineering
3. Model Training
4. Evaluation
5. Deployment Preparation
"""

# ============================================================================
# SECTION 1: SETUP AND IMPORTS
# ============================================================================

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import warnings
warnings.filterwarnings('ignore')

# Set display options
pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

# Set plot style
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")

print("="*70)
print("CREDIT CARD FRAUD DETECTION SYSTEM")
print("="*70)
print("\nAll libraries imported successfully!")

# ============================================================================
# SECTION 2: LOAD DATA
# ============================================================================

print("\n" + "="*70)
print("SECTION 2: DATA LOADING")
print("="*70)

# Load the credit card fraud dataset
# Download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud
df = pd.read_csv('data/creditcard.csv')

print(f"\nDataset shape: {df.shape}")
print(f"Total transactions: {len(df):,}")
print(f"Total features: {len(df.columns)}")

# Basic info
print("\nDataset Info:")
print(df.info())

print("\nFirst few rows:")
print(df.head())

# ============================================================================
# SECTION 3: EXPLORATORY DATA ANALYSIS
# ============================================================================

print("\n" + "="*70)
print("SECTION 3: EXPLORATORY DATA ANALYSIS")
print("="*70)

# Class distribution
fraud_count = df['Class'].value_counts()
print("\nClass Distribution:")
print(fraud_count)
print(f"\nFraud percentage: {df['Class'].mean()*100:.4f}%")

# Visualize class imbalance
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Count plot
axes[0].bar(['Legitimate', 'Fraud'], fraud_count.values, color=['#2ecc71', '#e74c3c'])
axes[0].set_ylabel('Count')
axes[0].set_title('Transaction Distribution')
axes[0].set_yscale('log')
for i, v in enumerate(fraud_count.values):
    axes[0].text(i, v, f'{v:,}', ha='center', va='bottom')

# Pie chart
colors = ['#2ecc71', '#e74c3c']
explode = (0, 0.1)
axes[1].pie(fraud_count.values, labels=['Legitimate', 'Fraud'], 
           autopct='%1.4f%%', startangle=90, colors=colors, explode=explode)
axes[1].set_title('Class Distribution (%)')

plt.tight_layout()
plt.show()

# Amount distribution
print("\nTransaction Amount Statistics:")
print(df.groupby('Class')['Amount'].describe())

# Visualize amount distribution
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Amount distribution by class
for class_val, label, color in [(0, 'Legitimate', '#2ecc71'), (1, 'Fraud', '#e74c3c')]:
    axes[0].hist(df[df['Class']==class_val]['Amount'], bins=50, 
                alpha=0.7, label=label, color=color)
axes[0].set_xlabel('Amount')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Transaction Amount Distribution')
axes[0].legend()
axes[0].set_xlim([0, 500])

# Box plot
df_plot = df.copy()
df_plot['Class'] = df_plot['Class'].map({0: 'Legitimate', 1: 'Fraud'})
sns.boxplot(data=df_plot, x='Class', y='Amount', ax=axes[1])
axes[1].set_title('Amount Distribution by Class')
axes[1].set_ylim([0, 300])

plt.tight_layout()
plt.show()

# Time analysis
print("\nTime Statistics:")
print(df['Time'].describe())

# Correlation of features with fraud
print("\nTop 10 Features Correlated with Fraud:")
correlations = df.corr()['Class'].abs().sort_values(ascending=False)
print(correlations.head(11))  # 11 because Class itself is included

# ============================================================================
# SECTION 4: FEATURE ENGINEERING
# ============================================================================

print("\n" + "="*70)
print("SECTION 4: FEATURE ENGINEERING")
print("="*70)

from src.feature_engineering import FraudFeatureEngine

# Initialize feature engine
feature_engine = FraudFeatureEngine()

# Create features
print("\nCreating engineered features...")
df_features = feature_engine.create_features(df, is_training=True)

print(f"\nOriginal features: {len(df.columns)}")
print(f"Engineered features: {len(df_features.columns)}")
print(f"New features added: {len(df_features.columns) - len(df.columns)}")

# Display new features
new_features = [col for col in df_features.columns if col not in df.columns]
print(f"\nNew features created ({len(new_features)}):")
for i, feat in enumerate(new_features, 1):
    print(f"{i:2d}. {feat}")

# Feature summary
print("\nFeature Summary Statistics:")
print(df_features[new_features].describe())

# ============================================================================
# SECTION 5: DATA PREPARATION
# ============================================================================

print("\n" + "="*70)
print("SECTION 5: DATA PREPARATION")
print("="*70)

# Separate features and target
X = df_features.drop(['Class', 'Time'], axis=1, errors='ignore')
y = df_features['Class']

print(f"\nFeature matrix shape: {X.shape}")
print(f"Target vector shape: {y.shape}")

# Handle any remaining missing values
X = X.fillna(X.median())

print(f"\nMissing values: {X.isna().sum().sum()}")

# ============================================================================
# SECTION 6: MODEL TRAINING
# ============================================================================

print("\n" + "="*70)
print("SECTION 6: MODEL TRAINING")
print("="*70)

from src.model_training import FraudModelTrainer

# Initialize trainer
trainer = FraudModelTrainer(random_state=42)

# Prepare data
X_train, X_test, y_train, y_test = trainer.prepare_data(X, y, test_size=0.3)

# Train XGBoost model
print("\n" + "-"*70)
print("Training XGBoost Classifier")
print("-"*70)
model = trainer.train_xgboost(X_train, y_train, use_resampling=True)

# Train Isolation Forest for anomaly detection
print("\n" + "-"*70)
print("Training Isolation Forest")
print("-"*70)
anomaly_model = trainer.train_isolation_forest(X_train, contamination=0.02)

# ============================================================================
# SECTION 7: MODEL EVALUATION
# ============================================================================

print("\n" + "="*70)
print("SECTION 7: MODEL EVALUATION")
print("="*70)

# Evaluate on test set
metrics = trainer.evaluate_model(X_test, y_test, model_name="XGBoost")

# Cross-validation
cv_results = trainer.cross_validate(X_train, y_train, cv_folds=5)

# Visualize results
print("\nGenerating evaluation plots...")
trainer.plot_evaluation_metrics(X_test, y_test, save_path='models/evaluation_plots.png')

# ============================================================================
# SECTION 8: PREDICTION EXAMPLES
# ============================================================================

print("\n" + "="*70)
print("SECTION 8: MAKING PREDICTIONS")
print("="*70)

from src.fraud_detection import FraudDetector

# Save models first
trainer.save_models()

# Load detector
detector = FraudDetector(
    model_path='models/xgboost_model.pkl',
    scaler_path='models/scaler.pkl',
    anomaly_model_path='models/isolation_forest.pkl'
)

# Test on sample transactions
print("\nTesting on sample transactions...")
sample_indices = [0, 1, 100, df[df['Class']==1].index[0]]  # Mix of legitimate and fraud
sample_transactions = X_test.iloc[sample_indices]

for idx, (original_idx, transaction) in enumerate(sample_transactions.iterrows()):
    print(f"\n--- Transaction {idx + 1} ---")
    result = detector.predict(transaction.to_frame().T)
    
    print(f"Fraud Probability: {result['fraud_probability']:.2%}")
    print(f"Risk Level: {result['risk_level']}")
    print(f"Recommendation: {result['recommendation']}")
    print(f"Actual Class: {'FRAUD' if y_test.loc[original_idx] == 1 else 'LEGITIMATE'}")

# Batch prediction example
print("\n" + "-"*70)
print("Batch Prediction Example")
print("-"*70)
batch_results = detector.predict_batch(X_test.head(100))
print(batch_results.head(10))

print(f"\nHigh-risk transactions in batch: {(batch_results['risk_level'] == 'HIGH').sum()}")
print(f"Critical-risk transactions in batch: {(batch_results['risk_level'] == 'CRITICAL').sum()}")

# ============================================================================
# SECTION 9: BUSINESS IMPACT ANALYSIS
# ============================================================================

print("\n" + "="*70)
print("SECTION 9: BUSINESS IMPACT ANALYSIS")
print("="*70)

# Calculate business metrics
y_pred = model.predict(X_test)
y_pred_proba = model.predict_proba(X_test)[:, 1]

tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()

# Assume average transaction amount for fraud
avg_fraud_amount = df[df['Class']==1]['Amount'].mean()
avg_legit_amount = df[df['Class']==0]['Amount'].mean()

# Calculate costs
# False Negative: Miss a fraud - lose the full amount
fn_cost = fn * avg_fraud_amount

# False Positive: Block legitimate - investigation cost + customer inconvenience
investigation_cost = 50  # Assumed cost per investigation
fp_cost = fp * investigation_cost

# True Positive: Catch a fraud - save the amount minus investigation
tp_benefit = tp * avg_fraud_amount - tp * investigation_cost

# Total impact
net_benefit = tp_benefit - fn_cost - fp_cost

print(f"\nBusiness Impact Analysis:")
print(f"{'='*50}")
print(f"Average Fraud Amount: ₹{avg_fraud_amount:.2f}")
print(f"Average Legitimate Amount: ₹{avg_legit_amount:.2f}")
print(f"\nModel Performance:")
print(f"  True Positives (Fraud Caught): {tp:,}")
print(f"  False Negatives (Fraud Missed): {fn:,}")
print(f"  False Positives (False Alarms): {fp:,}")
print(f"  True Negatives (Correctly Cleared): {tn:,}")
print(f"\nFinancial Impact:")
print(f"  Fraud Prevented: ₹{tp * avg_fraud_amount:,.2f}")
print(f"  Fraud Losses: ₹{fn * avg_fraud_amount:,.2f}")
print(f"  Investigation Costs: ₹{(tp + fp) * investigation_cost:,.2f}")
print(f"  Net Benefit: ₹{net_benefit:,.2f}")

# Calculate effectiveness
fraud_detection_rate = tp / (tp + fn) * 100
false_alarm_rate = fp / (fp + tn) * 100

print(f"\nOperational Metrics:")
print(f"  Fraud Detection Rate: {fraud_detection_rate:.2f}%")
print(f"  False Alarm Rate: {false_alarm_rate:.4f}%")

# ============================================================================
# SECTION 10: DEPLOYMENT PREPARATION
# ============================================================================

print("\n" + "="*70)
print("SECTION 10: DEPLOYMENT PREPARATION")
print("="*70)

print("\nModel artifacts saved:")
print("  ✓ models/xgboost_model.pkl")
print("  ✓ models/scaler.pkl")
print("  ✓ models/isolation_forest.pkl")
print("  ✓ models/evaluation_plots.png")

print("\nNext Steps for Deployment:")
print("  1. Run dashboard: streamlit run dashboard/app.py")
print("  2. Test API endpoints")
print("  3. Set up monitoring and alerts")
print("  4. Implement feedback loop for model retraining")
print("  5. Deploy to production environment")

print("\n" + "="*70)
print("PIPELINE COMPLETED SUCCESSFULLY!")
print("="*70)

# ============================================================================
# SECTION 11: SAVE FEATURE IMPORTANCE
# ============================================================================

print("\n" + "="*70)
print("SECTION 11: FEATURE ANALYSIS")
print("="*70)

# Save feature importance
feature_imp_df = trainer.feature_importance
feature_imp_df.to_csv('models/feature_importance.csv', index=False)

print("\nTop 20 Most Important Features:")
print(feature_imp_df.head(20).to_string(index=False))

# Visualize feature importance
plt.figure(figsize=(10, 8))
top_n = 20
top_features = feature_imp_df.head(top_n)
plt.barh(range(top_n), top_features['importance'])
plt.yticks(range(top_n), top_features['feature'])
plt.xlabel('Importance Score')
plt.title(f'Top {top_n} Feature Importances')
plt.gca().invert_yaxis()
plt.tight_layout()
plt.savefig('models/feature_importance_plot.png', dpi=300, bbox_inches='tight')
plt.show()

print("\nFeature importance plot saved to: models/feature_importance_plot.png")

# ============================================================================
# SUMMARY STATISTICS
# ============================================================================

print("\n" + "="*70)
print("FINAL SUMMARY")
print("="*70)

summary = f"""
Model Performance Summary:
--------------------------
ROC-AUC Score: {metrics['roc_auc']:.4f}
Precision: {metrics['precision']:.4f}
Recall: {metrics['recall']:.4f}
F1-Score: {metrics['f1_score']:.4f}
False Positive Rate: {metrics['false_positive_rate']:.4f}

Dataset Information:
-------------------
Total Transactions: {len(df):,}
Fraudulent Transactions: {fraud_count[1]:,} ({fraud_count[1]/len(df)*100:.4f}%)
Training Samples: {len(X_train):,}
Test Samples: {len(X_test):,}
Total Features: {X.shape[1]}

Business Impact:
---------------
Frauds Detected: {tp:,} / {tp + fn:,} ({fraud_detection_rate:.2f}%)
False Alarm Rate: {false_alarm_rate:.4f}%
Estimated Annual Savings: ₹{net_benefit * 365:,.2f}

The fraud detection system is ready for deployment!
"""

print(summary)

# Save summary
with open('models/model_summary.txt', 'w') as f:
    f.write(summary)

print("\nSummary saved to: models/model_summary.txt")
